{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers  as  klayers\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, LSTM, Input, Embedding, GlobalAveragePooling1D, Concatenate, Activation, Lambda, \\\n",
    "    BatchNormalization, Convolution1D, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quadratic_weighted_kappa import Conf_matrix, histogram\n",
    "class Neural_Tensor_layer(Layer):\n",
    "    def __init__(self, output_dim, input_dim=None, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        if self.input_dim:\n",
    "            kwargs['input_shape'] = (self.input_dim,)\n",
    "        super(Neural_Tensor_layer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        e1 = inputs[0]\n",
    "        e2 = inputs[1]\n",
    "        batch_size = K.shape(e1)[0]\n",
    "        k = self.output_dim\n",
    "\n",
    "        feed_forward = K.dot(K.concatenate([e1, e2]), self.V)\n",
    "\n",
    "        bilinear_tensor_products = [K.sum((e2 * K.dot(e1, self.W[0])) + self.b, axis=1)]\n",
    "\n",
    "        for i in range(k)[1:]:\n",
    "            btp = K.sum((e2 * K.dot(e1, self.W[i])) + self.b, axis=1)\n",
    "            bilinear_tensor_products.append(btp)\n",
    "\n",
    "        result = K.tanh(K.reshape(K.concatenate(bilinear_tensor_products, axis=0), (batch_size, k)) + feed_forward)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        mean = 0.0\n",
    "        std = 1.0\n",
    "        k = self.output_dim\n",
    "        d = self.input_dim\n",
    "        ##truncnorm generate continuous random numbers in given range\n",
    "        W_val = stats.truncnorm.rvs(-2 * std, 2 * std, loc=mean, scale=std, size=(k, d, d))\n",
    "        V_val = stats.truncnorm.rvs(-2 * std, 2 * std, loc=mean, scale=std, size=(2 * d, k))\n",
    "        self.W = K.variable(W_val)\n",
    "        self.V = K.variable(V_val)\n",
    "        self.b = K.zeros((self.input_dim,))\n",
    "        self.trainable_weights = [self.W, self.V, self.b]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        batch_size = input_shape[0][0]\n",
    "        return batch_size, self.output_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Temporal_Mean_Pooling(Layer):  # conversion from (samples,timesteps,features) to (samples,features)\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Temporal_Mean_Pooling, self).__init__(**kwargs)\n",
    "        # masked values in x (number_of_samples,time)\n",
    "        self.supports_masking = True\n",
    "        # Specifies number of dimensions to each layer\n",
    "        self.input_spec = InputSpec(ndim=3)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if mask is None:\n",
    "            mask = K.mean(K.ones_like(x), axis=-1)\n",
    "\n",
    "        mask = K.cast(mask, K.floatx())\n",
    "        # dimension size single vec/number of samples\n",
    "        return K.sum(x, axis=-2) / K.sum(mask, axis=-1, keepdims=True)\n",
    "\n",
    "    def compute_mask(self, input, mask):\n",
    "        return None\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QWK(y_true,y_pred,min_rating=None, max_rating=None):\n",
    "\tif min_rating is None:\n",
    "\t\tmin_rating=K.min(y_true)\n",
    "\t\t# min_rating=K.min(K.min(y_true),K.min(y_pred))\n",
    "\tif max_rating is None:\n",
    "\t\tmax_rating=K.max(y_true)\n",
    "\t\t# max_rating=K.max(K.max(y_true),K.max(y_pred))\n",
    "\n",
    "\tconf_matrix= Conf_matrix(y_true,y_pred,K.eval(min_rating),K.eval(max_rating))\n",
    "\n",
    "\thist_a=histogram(y_true,K.eval(min_rating),K.eval(max_rating))\n",
    "\thist_b=histogram(y_pred,K.eval(min_rating),K.eval(max_rating))\n",
    "\n",
    "\tnum=0.0;denom=0.0\n",
    "\n",
    "\tnum_ratings=len(conf_matrix)\n",
    "\tnum_items=float(len(K.eval(y_true)))\n",
    "\n",
    "\tfor i in range(num_ratings):\n",
    "\t\tfor j in range(num_ratings):\n",
    "\n",
    "\t\t\texpected_count=(hist_a[i]*hist_b[j]/num_items)\n",
    "\n",
    "\t\t\td=pow(i-j,2.0)/pow(num_ratings-1,2.0)\n",
    "\t\t\tnum+=d*conf_matrix[i][j]/num_items\n",
    "\t\t\tdenom+=d*expected_count/num_items\n",
    "\n",
    "\treturn K.variable(1.0 - num/denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 4000\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "VALIDATION_SPLIT = 0.20\n",
    "DELTA = 20\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "sentences = []\n",
    "\n",
    "originals = []\n",
    "\n",
    "fp1 = open(\"Resources/glove.6B.300d.txt\", \"r\", encoding='utf-8')\n",
    "glove_emb = {}\n",
    "for line in fp1:\n",
    "    temp = line.split(\" \")\n",
    "    glove_emb[temp[0]] = np.asarray([float(i) for i in temp[1:]])\n",
    "\n",
    "print(\"Embedding done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_type = '4'\n",
    "\n",
    "fp = open(\"Resources/training_set_rel32.tsv\", 'r', encoding=\"ascii\", errors=\"ignore\")\n",
    "fp.readline()\n",
    "for line in fp:\n",
    "    temp = line.split(\"\\t\")\n",
    "    if temp[1] == essay_type:  # why only 4 ?? - evals in prompt specific fashion\n",
    "        originals.append(float(temp[6]))\n",
    "fp.close()\n",
    "\n",
    "print(\"range min - \", min(originals), \" ; range max - \", max(originals))\n",
    "\n",
    "range_min = min(originals)\n",
    "range_max = max(originals)\n",
    "\n",
    "fp = open(\"Resources/training_set_rel32.tsv\", 'r', encoding=\"ascii\", errors=\"ignore\")\n",
    "fp.readline()\n",
    "sentences = []\n",
    "for line in fp:\n",
    "    temp = line.split(\"\\t\")\n",
    "    if temp[1] == essay_type:  # why only 4 ?? - evals in prompt specific fashion\n",
    "        texts.append(temp[2])\n",
    "        labels.append((float(temp[6]) - range_min) / (range_max - range_min))  # why ??  - normalize to range [0-1]\n",
    "        line = temp[2].strip()\n",
    "        sentences.append(nltk.tokenize.word_tokenize(line))\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"text labels appended %s\" % len(texts))\n",
    "\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sentences:\n",
    "    temp1 = np.zeros((1, EMBEDDING_DIM))\n",
    "    for w in i:\n",
    "        if w in glove_emb:\n",
    "            temp1 += glove_emb[w]\n",
    "    temp1 /= len(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()  # num_words=MAX_NB_WORDS) #limits vocabulary size\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)  # returns list of sequences\n",
    "word_index = tokenizer.word_index   # dictionary mapping\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trad_feat_std = StandardScaler()\n",
    "\n",
    "\"\"\"trad_feats = trad_feat_std.fit_transform(data[[\n",
    "    \"num_tokens\", \"num_sentences\", \"average_sent_len\", \"average_word_len\", \"num_long_words\", \"num_stopwords\",\n",
    "    \"num_characters\", \"num_commas\", \"num_quotations\", \"num_exclamation_marks\", \"f_score\", \"type_token_ratio\",\n",
    "    \"avg_word_freq\", \"cohesion\", \"one_gram_overlap\", \"two_gram_overlap\", \"three_gram_overlap\", \"spelling_error\",\n",
    "    \"grammar_error\", \"syllable_count\", \"flesch_reading_ease\", \"flesch_kincaid_grade\", \"gunning_fog\", \"smog_index\",\n",
    "    \"automated_readability_index\", \"coleman_liau_index\", \"linsear_write_formula\", \"dale_chall_readability_score\",\n",
    "    \"difficult_words\", \"neg_sentiment\", \"neu_sentiment\", \"pos_sentiment\"\n",
    "]].values)\"\"\"\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "#trad_feats = trad_feats[indices]\n",
    "validation_size = int(VALIDATION_SPLIT * data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:-validation_size]\n",
    "y_train = labels[:-validation_size]\n",
    "#trad_feats = trad_feats[:-validation_size]\n",
    "\n",
    "#trad_feats_val = trad_feats[-validation_size:]\n",
    "x_val = data[-validation_size:]\n",
    "y_val = labels[-validation_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word_index.items():\n",
    "    if i >= len(word_index):\n",
    "        continue\n",
    "    if word in glove_emb:\n",
    "        embedding_matrix[i] = glove_emb[word]\n",
    "vocab_size = len(word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(vocab_size, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            mask_zero=True,\n",
    "                            trainable=False)\n",
    "side_embedding_layer = Embedding(vocab_size, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                                 input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                 mask_zero=False,\n",
    "                                 trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SKIPFLOW(lstm_dim=50, lr=1e-4, lr_decay=1e-6, k=5, eta=3, delta=50, activation=\"relu\", maxlen=MAX_SEQUENCE_LENGTH,\n",
    "             seed=None):\n",
    "    e = Input(name='essay', shape=(maxlen,))\n",
    "\n",
    "    trad_feats = Input(shape=(7,))\n",
    "    embed = embedding_layer(e)\n",
    "\n",
    "    lstm_layer = LSTM(lstm_dim, return_sequences=True)\n",
    "\n",
    "    hidden_states = lstm_layer(embed)\n",
    "\n",
    "    htm = Temporal_Mean_Pooling()(hidden_states)\n",
    "\n",
    "    side_embed = side_embedding_layer(e)\n",
    "    side_hidden_states = lstm_layer(side_embed)\n",
    "\n",
    "    tensor_layer = Neural_Tensor_layer(output_dim=k, input_dim=lstm_dim)\n",
    "\n",
    "    pairs = [((eta + i * delta) % maxlen, (eta + i * delta + delta) % maxlen) for i in range(maxlen // delta)]\n",
    "    hidden_pairs = [\n",
    "        (Lambda(lambda t: t[:, p[0], :])(side_hidden_states), Lambda(lambda t: t[:, p[1], :])(side_hidden_states)) for p\n",
    "        in pairs]\n",
    "\n",
    "    sigmoid = Dense(1, activation=\"sigmoid\", kernel_initializer=initializers.glorot_normal(seed=seed))\n",
    "\n",
    "    coherence = [sigmoid(tensor_layer([hp[0], hp[1]])) for hp in hidden_pairs]\n",
    "\n",
    "    co_tm = Concatenate()(coherence[:] + [htm])\n",
    "\n",
    "    dense = Dense(256, activation=activation, kernel_initializer=initializers.glorot_normal(seed=seed))(co_tm)\n",
    "\n",
    "    dense = Dense(128, activation=activation, kernel_initializer=initializers.glorot_normal(seed=seed))(dense)\n",
    "    dense = Dense(64, activation=activation, kernel_initializer=initializers.glorot_normal(seed=seed))(dense)\n",
    "    out = Dense(1, activation=\"sigmoid\")(dense)\n",
    "\n",
    "    model = Model(inputs=[e], outputs=[out])\n",
    "    adam = Adam(lr=lr, decay=lr_decay)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=adam, metrics=[\"MSE\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping = EarlyStopping(monitor=\"val_mean_squared_error\", patience=5)\n",
    "sf_1 = SKIPFLOW(lstm_dim=50, lr=2e-4, lr_decay=2e-6, k=4, eta=13, delta=50, activation=\"relu\", seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "# epochs = 1000\n",
    "hist = sf_1.fit([x_train], y_train, batch_size=1024, epochs=epochs,\n",
    "                validation_data=([x_val], y_val), callbacks=[earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sf_1.predict([x_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_fin = [int(round(a * (range_max - range_min) + range_min)) for a in y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_fin = [int(round(a * (range_max - range_min) + range_min)) for a in y_pred.reshape(58).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cohen_kappa_score(y_val_fin, y_pred_fin, weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_1.save('4_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred * (range_max - range_min) + range_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(sf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_1.save_weights('4_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
